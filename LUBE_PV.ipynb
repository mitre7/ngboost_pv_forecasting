{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "import time\n",
    "\n",
    "import properscoring as prscore\n",
    "\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('power_weather_data.csv')\n",
    "\n",
    "# csv file MUST contain 'date' and 'Power' fields\n",
    "# optional: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].apply(lambda x: x.hour )\n",
    "df['month'] = df['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['hour_sin'] = np.sin(df['hour'] * 2 * np.pi/24)\n",
    "# df['hour_cos'] = np.cos(df['hour'] * 2 * np.pi/24)\n",
    "df['month_sin'] = np.sin(df['month'] * 2 * np.pi/12)\n",
    "df['month_cos'] = np.cos(df['month'] * 2 * np.pi/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['hour']>=6) & (df['hour']<=21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['hour', 'month'], axis=1)\n",
    "df = df.drop(['month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = df['Power']\n",
    "\n",
    "PowerData = pd.concat([P.shift(3), P.shift(2), P.shift(1)], axis=1)\n",
    "PowerData.columns = ['t-45', 't-30', 't-15']\n",
    "\n",
    "df = pd.concat([df, PowerData.reindex(df.index)], axis=1)\n",
    "    \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = [['2018-03-01', '2019-03-15']]\n",
    "\n",
    "val_days = 14\n",
    "\n",
    "# n_points_day = 4 * 24\n",
    "n_points_day = 4 * 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for w in weeks:\n",
    "    \n",
    "    w_start = datetime.strptime(w[0]+\" 00:00\", '%Y-%m-%d %H:%M')\n",
    "    w_end = datetime.strptime(w[1]+\" 23:59\", '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    dfs.append(df[(df['date'] > w_start) & (df['date'] < w_end)])\n",
    "    \n",
    "n_sets = len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = []\n",
    "X_test_ = []\n",
    "y_train_ = []\n",
    "y_test_ = []\n",
    "\n",
    "x_scaler = []\n",
    "y_scaler = []\n",
    "\n",
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    train = dfs[i][:int(-n_points_day*val_days)]\n",
    "    test = dfs[i][int(-n_points_day*val_days):]\n",
    "    \n",
    "    X_tr = train.drop(['Power','date'], axis=1).values\n",
    "    X_t = test.drop(['Power','date'], axis=1).values\n",
    "    \n",
    "    y_tr = train['Power'].values\n",
    "    y_t = test['Power'].values\n",
    "    \n",
    "    x_sc = MinMaxScaler()\n",
    "    y_sc = MinMaxScaler()\n",
    "#     x_sc = StandardScaler()\n",
    "#     y_sc = StandardScaler()\n",
    "    x_sc.fit(X_tr)\n",
    "    y_sc.fit(y_tr.reshape(-1, 1))\n",
    "    x_scaler.append(x_sc)\n",
    "    y_scaler.append(y_sc)\n",
    "    \n",
    "    X_train_.append(x_sc.transform(X_tr))\n",
    "    X_test_.append(x_sc.transform(X_t))\n",
    "    y_train_.append(y_sc.transform(y_tr.reshape(-1, 1)) + 0.001)\n",
    "    y_test_.append(y_sc.transform(y_t.reshape(-1, 1)) + 0.001)\n",
    "    \n",
    "    t_train.append(dfs[i].iloc[:int(-n_points_day*val_days)]['date'].values)\n",
    "    t_test.append(dfs[i].iloc[int(-n_points_day*val_days):]['date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    X_train.append(torch.from_numpy(X_train_[i]).float())\n",
    "    X_test.append(torch.from_numpy(X_test_[i]).float())\n",
    "    \n",
    "    y_tr = torch.from_numpy(y_train_[i]).float()\n",
    "    y_train.append(torch.squeeze(y_tr))\n",
    "    y_t = torch.from_numpy(y_test_[i]).float()\n",
    "    y_test.append(torch.squeeze(y_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_neurons = 50\n",
    "eta = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, n_features):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "    self.fc2 = nn.Linear(n_neurons, 2)\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x)) #\n",
    "    return torch.sigmoid(self.fc2(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CWC(y_pred, y_true):\n",
    "    \n",
    "    y_pred = Variable(y_pred, requires_grad=True).to(device)\n",
    "    y_true = Variable(y_true, requires_grad=True).to(device)\n",
    "    \n",
    "    u = y_pred.detach().numpy().T[0]\n",
    "    l = y_pred.detach().numpy().T[1]\n",
    "    \n",
    "    u = torch.squeeze(torch.from_numpy(u).float())\n",
    "    l = torch.squeeze(torch.from_numpy(l).float())\n",
    "   \n",
    "    sum = 0\n",
    "    W = []\n",
    "    for i in range(len(y_pred)):\n",
    "        \n",
    "        Wi = torch.abs(u[i]-l[i]) #)**2 \n",
    "        W.append(Wi)\n",
    "        \n",
    "        if l[i] < y_true[i] < u[i]:\n",
    "            sum += 1\n",
    "    \n",
    "    #calculate PICP: PI coverage probability\n",
    "    PICP = sum/len(y_true)\n",
    "    \n",
    "    #calculate MPIW\n",
    "    W = np.array(W)\n",
    "    W = torch.from_numpy(W).float()\n",
    "    MPIW = torch.sqrt(torch.mean(W))\n",
    "    \n",
    "    R = torch.max(y_true)-torch.min(y_true)\n",
    "\n",
    "    return ((MPIW)/R)*(1+1*math.exp(-eta*(PICP-0.95)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler(object):\n",
    "    def __init__(self, minval, maxval, dtype='float', cuda=False):\n",
    "        self.minval = minval\n",
    "        self.maxval = maxval\n",
    "        self.cuda = cuda\n",
    "        self.dtype_str = dtype\n",
    "        dtypes = {\n",
    "            'float': torch.cuda.FloatTensor if cuda else torch.FloatTensor,\n",
    "            'int': torch.cuda.IntTensor if cuda else torch.IntTensor,\n",
    "            'long': torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        }\n",
    "        self.dtype = dtypes[dtype]\n",
    "\n",
    "    def sample(self, size):\n",
    "        if self.dtype_str == 'float':\n",
    "            return self.dtype(*size).uniform_(\n",
    "                self.minval, self.maxval\n",
    "            )\n",
    "        elif self.dtype_str == 'int' or self.dtype_str == 'long':\n",
    "            return self.dtype(*size).random_(\n",
    "                self.minval, self.maxval + 1\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"unknown dtype\")\n",
    "\n",
    "\n",
    "class GaussianSampler(object):\n",
    "    def __init__(self, mu, sigma, dtype='float', cuda=False):\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu\n",
    "        self.cuda = cuda\n",
    "        self.dtype_str = dtype\n",
    "        dtypes = {\n",
    "            'float': torch.cuda.FloatTensor if cuda else torch.FloatTensor,\n",
    "            'int': torch.cuda.IntTensor if cuda else torch.IntTensor,\n",
    "            'long': torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        }\n",
    "        self.dtype = dtypes[dtype]\n",
    "\n",
    "    def sample(self, size):\n",
    "        ''' pytorch doesnt support int or long normal distrs\n",
    "            so we will resolve to casting '''\n",
    "        rand_float = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        rand_block = rand_float(*size).normal_(self.mu, self.sigma)\n",
    "\n",
    "        if self.dtype_str == 'int' or self.dtype_str == 'long':\n",
    "            rand_block = rand_block.type(self.dtype)\n",
    "\n",
    "        return rand_block\n",
    "\n",
    "\n",
    "class SimulatedAnnealing(Optimizer):\n",
    "    def __init__(self, params, sampler, tau0=5.0, anneal_rate=0.0003,\n",
    "                 min_temp=1e-5, anneal_every=10, hard=True, hard_rate=0.95):\n",
    "        defaults = dict(sampler=sampler, tau0=tau0, tau=tau0, anneal_rate=anneal_rate,\n",
    "                        min_temp=min_temp, anneal_every=anneal_every,\n",
    "                        hard=hard, hard_rate=hard_rate, iteration=0)\n",
    "        super(SimulatedAnnealing, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        if closure is None:\n",
    "            raise Exception(\"loss closure is required to do SA\")\n",
    "\n",
    "        loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # the sampler samples randomness\n",
    "            # that is used in optimizations\n",
    "            sampler = group['sampler']\n",
    "\n",
    "            # clone all of the params to keep in case we need to swap back\n",
    "            cloned_params = [p.clone() for p in group['params']]\n",
    "\n",
    "            for p in group['params']:\n",
    "                # anneal tau if it matches the requirements\n",
    "                if group['iteration'] > 0 and group['iteration'] % group['anneal_every'] == 0:\n",
    "                    if not group['hard']:\n",
    "                        # smoother annealing: consider using this over hard annealing\n",
    "                        rate = -group['anneal_rate'] * group['iteration']\n",
    "                        group['tau'] = np.maximum(group['tau0'] * np.exp(rate),\n",
    "                                                  group['min_temp'])\n",
    "                    else:\n",
    "                        # hard annealing\n",
    "                        group['tau'] = np.maximum(group['hard_rate'] * group['tau'],\n",
    "                                                  group['min_temp'])\n",
    "\n",
    "                random_perturbation = group['sampler'].sample(p.data.size())\n",
    "                p.data = p.data / torch.norm(p.data)\n",
    "                p.data.add_(random_perturbation)\n",
    "                group['iteration'] += 1\n",
    "\n",
    "            # re-evaluate the loss function with the perturbed params\n",
    "            # if we didn't accept the new params, then swap back and return\n",
    "            loss_perturbed = closure()\n",
    "            final_loss, is_swapped_back = self.anneal(loss, loss_perturbed, group['tau'])\n",
    "            if is_swapped_back:\n",
    "                for p, pbkp in zip(group['params'], cloned_params):\n",
    "                    p.data = pbkp.data\n",
    "\n",
    "            return final_loss \n",
    "\n",
    "\n",
    "    def anneal(self, loss, loss_perturbed, tau):\n",
    "        '''returns loss, is_new_loss'''\n",
    "        def acceptance_prob(old, new, temp):\n",
    "            return torch.exp((old - new)/(temp))\n",
    "\n",
    "        if loss_perturbed.data < loss.data:\n",
    "#             print(\"old = \", loss.data, \"| pert = \", loss_perturbed.data, \" | tau = \", tau)\n",
    "            return loss_perturbed, False\n",
    "        else:\n",
    "            # evaluate the metropolis criterion\n",
    "            ap = acceptance_prob(loss, loss_perturbed, tau)\n",
    "            random = np.random.rand()\n",
    "            print(\"old = \", loss.data, \"| new = \", loss_perturbed.data,\n",
    "                  \" | ap = \", ap.data, \" | tau = \", tau, \" | r = \", random)\n",
    "            \n",
    "            if ap.data > random:\n",
    "                return loss_perturbed, False\n",
    "\n",
    "            return loss, True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    y_pred_train = net(X_train_i)\n",
    "    loss = CWC(y_pred_train, y_train_i)\n",
    "    return loss\n",
    "\n",
    "t_loss = []\n",
    "nets = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    net = Net(X_train[i].shape[1])\n",
    "\n",
    "    sampler = GaussianSampler(mu=0, sigma=1) #sampler = UniformSampler(minval=-0.5, maxval=0.5)\n",
    "    optimizer = SimulatedAnnealing(net.parameters(), sampler=sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train_i = X_train[i].to(device)\n",
    "    y_train_i = y_train[i].to(device)\n",
    "    X_test_i = X_test[i].to(device)\n",
    "    y_test_i = y_test[i].to(device)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    ite = []\n",
    "    loss_all = []\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        y_pred_train = net(X_train_i)\n",
    "        y_pred_train = torch.squeeze(y_pred_train)\n",
    "        train_loss = CWC(y_pred_train, y_train_i)\n",
    "        train_loss = train_loss.to(device)\n",
    "\n",
    "\n",
    "        ite = np.append(ite, epoch)\n",
    "        loss_all = np.append(loss_all, train_loss.detach().numpy()) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "    \n",
    "    t_loss.append(loss_all)\n",
    "    nets.append(net)\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PICP_func(y, lower, upper):\n",
    "    sum_points = 0\n",
    "    for i, yi in enumerate(y):\n",
    "        if lower[i] <= yi <= upper[i]:\n",
    "            sum_points += 1\n",
    "    \n",
    "    return sum_points / len(y)\n",
    "\n",
    "def PINAW_func(y, lower, upper):\n",
    "    PIAW = np.mean(upper - lower)\n",
    "    R = np.max(y) - np.min(y)\n",
    "    PINAW = PIAW / R\n",
    "    \n",
    "    return PINAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    \n",
    "    net = nets[i]\n",
    "    \n",
    "    y_pred_train = net(X_train[i])\n",
    "    y_pred_train= y_pred_train.detach().numpy()\n",
    "    \n",
    "    y_train_i = y_train[i].cpu()\n",
    "    y_train_i = y_train_i.detach().numpy()\n",
    "    \n",
    "    \n",
    "    # For multi-step ahead prediction\n",
    "    y_45_ = net(X_test[i][0].unsqueeze(0)).detach().numpy()\n",
    "    y_45 = ((y_45_.T[0] + y_45_.T[1]) / 2)[0]\n",
    "    y_30_ = net(X_test[i][1].unsqueeze(0)).detach().numpy()\n",
    "    y_30 = ((y_30_.T[0] + y_30_.T[1]) / 2)[0]\n",
    "    y_15_ = net(X_test[i][2].unsqueeze(0)).detach().numpy()\n",
    "    y_15 = ((y_15_.T[0] + y_15_.T[1]) / 2)[0]\n",
    "    for j in range(3, X_test[i].shape[0]):\n",
    "        X_test[i][j][-3] = torch.tensor(y_45)\n",
    "        X_test[i][j][-2] = torch.tensor(y_30)\n",
    "        X_test[i][j][-1] = torch.tensor(y_15)\n",
    "        y_pred_j_ = net(X_test[i][j].unsqueeze(0)).detach().numpy()\n",
    "        y_pred_j = ((y_pred_j_.T[0] + y_pred_j_.T[1]) / 2)[0]\n",
    "        y_45 = y_30\n",
    "        y_30 = y_15\n",
    "        y_15 = y_pred_j\n",
    "    # end of multi-step ahead\n",
    "    \n",
    "    y_pred_test = net(X_test[i])\n",
    "    y_pred_test= y_pred_test.detach().numpy()\n",
    "    y_test_i = y_test[i].cpu()\n",
    "    y_test_i = y_test_i.detach().numpy()\n",
    "    \n",
    "    upper_train = y_pred_train.T[0]\n",
    "    lower_train = y_pred_train.T[1]\n",
    "    \n",
    "    upper = y_pred_test.T[0]\n",
    "    lower = y_pred_test.T[1]\n",
    "    \n",
    "    real_y_train = y_scaler[i].inverse_transform(y_train_i.reshape(-1, 1))\n",
    "    real_y_test = y_scaler[i].inverse_transform(y_test_i.reshape(-1, 1))\n",
    "    \n",
    "    upper_train = y_scaler[i].inverse_transform(upper_train.reshape(-1, 1))\n",
    "    lower_train = y_scaler[i].inverse_transform(lower_train.reshape(-1, 1))\n",
    "    \n",
    "    upper = y_scaler[i].inverse_transform(upper.reshape(-1, 1))\n",
    "    lower = y_scaler[i].inverse_transform(lower.reshape(-1, 1))\n",
    "    \n",
    "    real_y_test = real_y_test.flatten()\n",
    "    real_y_train = real_y_train.flatten()\n",
    "    \n",
    "    lower_train = lower_train.flatten()\n",
    "    upper_train = upper_train.flatten()\n",
    "    \n",
    "    lower = lower.flatten()\n",
    "    upper = upper.flatten()\n",
    "    \n",
    "    for j in range(len(lower)):\n",
    "        if lower[j]<10e-6:\n",
    "            lower[j]=0\n",
    "        \n",
    "    mean = (upper+lower)/2\n",
    "    std = (mean - lower)/1.96\n",
    "    \n",
    "    # Deterministic metrics\n",
    "    MAE = mean_absolute_error(real_y_test, mean)\n",
    "    RMSE = mean_squared_error(real_y_test, mean, squared=False)\n",
    "    MBE = np.mean(mean - real_y_test)\n",
    "    print(f'MAE: {MAE:.3f}')\n",
    "    print(f'RMSE: {RMSE:.3f}')\n",
    "    print(f'MBE: {MBE:.3f}')\n",
    "    \n",
    "    # Probabilistic metrics\n",
    "    PICP = PICP_func(real_y_test, lower, upper)\n",
    "    PINAW = PINAW_func(real_y_test, lower, upper)\n",
    "    C = prscore.crps_gaussian(real_y_test, mu=mean, sig=std)\n",
    "    CRPS = C.mean()\n",
    "    print(f'PICP: {PICP:.3f}')\n",
    "    print(f'PINAW: {PINAW:.3f}')\n",
    "    print(f'CRPS: {CRPS:.3f}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
