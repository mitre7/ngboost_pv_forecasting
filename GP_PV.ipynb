{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel as RBF\n",
    "from gpytorch.kernels import ScaleKernel as C\n",
    "from gpytorch.kernels import PeriodicKernel as Per\n",
    "from gpytorch.kernels import RQKernel as RQ\n",
    "from gpytorch.kernels import MaternKernel as M\n",
    "from gpytorch.kernels import PolynomialKernel\n",
    "\n",
    "import properscoring as prscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('power_weather_data.csv')\n",
    "\n",
    "# csv file MUST contain 'date' and 'Power' fields\n",
    "# optional: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].apply(lambda x: x.hour )\n",
    "df['month'] = df['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['hour_sin'] = np.sin(df['hour'] * 2 * np.pi/24)\n",
    "# df['hour_cos'] = np.cos(df['hour'] * 2 * np.pi/24)\n",
    "df['month_sin'] = np.sin(df['month'] * 2 * np.pi/12)\n",
    "df['month_cos'] = np.cos(df['month'] * 2 * np.pi/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['hour']>=6) & (df['hour']<=21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['hour', 'month'], axis=1)\n",
    "df = df.drop(['month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = df['Power']\n",
    "\n",
    "PowerData = pd.concat([P.shift(3), P.shift(2), P.shift(1)], axis=1)\n",
    "PowerData.columns = ['t-45', 't-30', 't-15']\n",
    "\n",
    "df = pd.concat([df, PowerData.reindex(df.index)], axis=1)\n",
    "    \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = [['2018-03-01', '2019-03-15']]\n",
    "\n",
    "val_days = 14\n",
    "\n",
    "# n_points_day = 4 * 24\n",
    "n_points_day = 4 * 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for w in weeks:\n",
    "    \n",
    "    w_start = datetime.strptime(w[0]+\" 00:00\", '%Y-%m-%d %H:%M')\n",
    "    w_end = datetime.strptime(w[1]+\" 23:59\", '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    dfs.append(df[(df['date'] > w_start) & (df['date'] < w_end)])\n",
    "    \n",
    "n_sets = len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = []\n",
    "X_test_ = []\n",
    "y_train_ = []\n",
    "y_test = []\n",
    "\n",
    "x_scaler = []\n",
    "y_scaler = []\n",
    "\n",
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    train = dfs[i][:int(-n_points_day*val_days)]\n",
    "    test = dfs[i][int(-n_points_day*val_days):]\n",
    "    \n",
    "    X_tr = train.drop(['Power','date'], axis=1).values\n",
    "    X_t = test.drop(['Power','date'], axis=1).values\n",
    "    \n",
    "    y_tr = train['Power'].values\n",
    "    y_t = test['Power'].values\n",
    "    \n",
    "    x_sc = MinMaxScaler()\n",
    "    y_sc = MinMaxScaler(feature_range=(-1,1))\n",
    "#     x_sc = StandardScaler()\n",
    "#     y_sc = StandardScaler()\n",
    "    x_sc.fit(X_tr)\n",
    "    y_sc.fit(y_tr.reshape(-1, 1))\n",
    "    x_scaler.append(x_sc)\n",
    "    y_scaler.append(y_sc)\n",
    "    \n",
    "    X_train_.append(x_sc.transform(X_tr))\n",
    "    X_test_.append(x_sc.transform(X_t))\n",
    "    y_train_.append(y_sc.transform(y_tr.reshape(-1, 1)))\n",
    "    y_test.append(y_t)\n",
    "    \n",
    "    t_train.append(dfs[i].iloc[:int(-n_points_day*val_days)]['date'].values)\n",
    "    t_test.append(dfs[i].iloc[int(-n_points_day*val_days):]['date'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    X_train.append(torch.from_numpy(X_train_[i]))\n",
    "    X_test.append(torch.from_numpy(X_test_[i]))\n",
    "    \n",
    "    y_tr = torch.from_numpy(y_train_[i])\n",
    "    y_train.append(torch.flatten(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X_train, y_train, likelihood):\n",
    "        super(ExactGPModel, self).__init__(X_train, y_train, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = C(RQ())    \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_iter = 1000\n",
    "train_loss = []\n",
    "\n",
    "models = []\n",
    "likelihoods = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    \n",
    "    print(i)\n",
    "    X_tr = X_train[i]\n",
    "    y_tr = y_train[i]\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(X_tr, y_tr, likelihood)\n",
    "\n",
    "    model = model.double()\n",
    "    likelihood = likelihood.double()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.07) \n",
    "\n",
    "    # Loss for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    ite = []\n",
    "    loss_all = []\n",
    "    \n",
    "    for j in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X_tr)\n",
    "        # Calculate loss and backprop gradients\n",
    "        loss = -mll(output, y_tr)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        ite = np.append(ite, j)\n",
    "        loss_all = np.append(loss_all, loss.detach().numpy())\n",
    "        \n",
    "    \n",
    "    train_loss.append(loss_all)\n",
    "    models.append(model)\n",
    "    likelihoods.append(likelihood)\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PICP_func(y, lower, upper):\n",
    "    sum_points = 0\n",
    "    for i, yi in enumerate(y):\n",
    "        if lower[i] <= yi <= upper[i]:\n",
    "            sum_points += 1\n",
    "    \n",
    "    return sum_points / len(y)\n",
    "\n",
    "def PINAW_func(y, lower, upper):\n",
    "    PIAW = np.mean(upper - lower)\n",
    "    R = np.max(y) - np.min(y)\n",
    "    PINAW = PIAW / R\n",
    "    \n",
    "    return PINAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # Unpacking\n",
    "    model = models[i]\n",
    "    likelihood = likelihoods[i]\n",
    "    X_t = X_test[i]\n",
    "    y_t = y_test[i]\n",
    "    x_sc = x_scaler[i]\n",
    "    y_sc = y_scaler[i]\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # For multi-step ahead prediction\n",
    "    y_45 = model(X_t[0].unsqueeze(0)).mean\n",
    "    y_30 = model(X_t[1].unsqueeze(0)).mean\n",
    "    y_15 = model(X_t[2].unsqueeze(0)).mean\n",
    "    for j in range(3, X_t.shape[0]):\n",
    "        X_t[j][-3] = y_45\n",
    "        X_t[j][-2] = y_30\n",
    "        X_t[j][-1] = y_15\n",
    "        y_pred_j = model(X_t[j].unsqueeze(0))\n",
    "        y_45 = y_30\n",
    "        y_30 = y_15\n",
    "        y_15 = y_pred_j.mean\n",
    "    # end of multi-step ahead\n",
    "    \n",
    "    y_pred_i = model(X_t)\n",
    "    f_pred_i = likelihood(model(X_t))\n",
    "    \n",
    "    y_pred = y_pred_i.mean\n",
    "    y_var = y_pred_i.variance\n",
    "    y_covar = y_pred_i.covariance_matrix\n",
    "    \n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    \n",
    "    real_y_pred = y_sc.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    \n",
    "    real_y_pred = real_y_pred.flatten()\n",
    "    real_y_test = y_t.flatten()\n",
    "    \n",
    "    lower, upper = f_pred_i.confidence_region()\n",
    "    \n",
    "    lower = lower.detach().numpy()\n",
    "    upper = upper.detach().numpy()\n",
    "    \n",
    "    lower = y_sc.inverse_transform(lower.reshape(-1, 1))\n",
    "    upper = y_sc.inverse_transform(upper.reshape(-1, 1))\n",
    "    \n",
    "    lower = lower.flatten()\n",
    "    upper = upper.flatten()\n",
    "    \n",
    "    mean = (upper+lower)/2\n",
    "    std = (mean - lower)/1.96\n",
    "    \n",
    "    # Deterministic metrics\n",
    "    MAE = mean_absolute_error(real_y_test, mean)\n",
    "    RMSE = mean_squared_error(real_y_test, mean, squared=False)\n",
    "    MBE = np.mean(mean - real_y_test)\n",
    "    print(f'MAE: {MAE:.3f}')\n",
    "    print(f'RMSE: {RMSE:.3f}')\n",
    "    print(f'MBE: {MBE:.3f}')\n",
    "    \n",
    "    # Probabilistic metrics\n",
    "    PICP = PICP_func(real_y_test, lower, upper)\n",
    "    PINAW = PINAW_func(real_y_test, lower, upper)\n",
    "    C = prscore.crps_gaussian(real_y_test, mu=mean, sig=std)\n",
    "    CRPS = C.mean()\n",
    "    print(f'PICP: {PICP:.3f}')\n",
    "    print(f'PINAW: {PINAW:.3f}')\n",
    "    print(f'CRPS: {CRPS:.3f}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
